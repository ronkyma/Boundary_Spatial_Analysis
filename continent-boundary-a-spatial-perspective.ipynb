{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook I used huge data set with complete 'geonames' which contains all stored by humans spots in the world. **Next, I ask three questions:**\n* \"**How are the biggest cities distributed?**\" - in chapter 3 I give the answer what are the trends in the large cities distribution\n* \"**What is continent according to DBSCAN?**\" - the question from the title is answered in the chapter 4. This topic is probably the most interesting in this notebook as the definition of continent remains very discussable\n* \"**How to approximate elevation and what we can read from it?**\" - in this last chapter 5 I try to properly answer the question how to approximate the elvationa nd plot it","metadata":{}},{"cell_type":"markdown","source":"# 2.Data preparation","metadata":{}},{"cell_type":"markdown","source":"First, I introduce useful libraries:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport math\nimport netCDF4\nimport gc\nfrom scipy.interpolate import griddata\nfrom osgeo import gdal\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn.preprocessing import StandardScaler\nfrom math import cos, asin, sqrt\nfrom numpy import nansum,nanmean,linspace,meshgrid\nfrom numpy import meshgrid\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib.colors import Normalize\n%matplotlib inline\n\nRandom_seed=123","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Second, I run the code to clean data set from [my other notebook](https://www.kaggle.com/jjmewtw/total-geo-analysis-spatial-cleaning-plotting):","metadata":{}},{"cell_type":"code","source":"def distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(a))\n\ndef round_up_to_even(f):\n    return math.ceil(f / 2.) * 2\n\nn = 11061987\ns = 11061987\nskip = sorted(random.sample(range(n),n-s))\n        \ndf_path = \"../input/geonames-database/geonames.csv\"\ndf = pd.read_csv(df_path,skiprows=skip,index_col=\"geonameid\")\nISO = pd.read_csv('../input/alpha-country-codes/Alpha__2_and_3_country_codes.csv', sep=';')\nPopulation = pd.read_csv('../input/population-by-country-2020/population_by_country_2020.csv')\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\ndf=df.drop(['alternatenames','admin2 code','admin3 code','admin4 code','cc2'], axis=1)\n\ndf[\"latitude_app\"] = df.apply(lambda row: round_up_to_even(row['latitude']),axis=1)\ndf[\"longitude_app\"] = df.apply(lambda row: round_up_to_even(row['longitude']),axis=1)\n\nelevation_table = df[['elevation','latitude_app','longitude_app']].groupby(['latitude_app',\n                'longitude_app']).agg({'elevation': lambda x: x.mean(skipna=True)}).sort_values(by=['latitude_app', \n                'longitude_app'], ascending=False).reset_index()\n\ndf = pd.merge(df,  elevation_table,  on =['latitude_app', 'longitude_app'],  how ='inner')\n\nWorldAverageElevation = 840\ndf['elevation_y']=df['elevation_y'].fillna(WorldAverageElevation)\n\ndf=df.drop(['elevation_x'], axis=1)\n\nISO['Country'] = ISO.apply(lambda row: str.rstrip(row['Country']),axis=1)\n\nISO_toMerge = ISO.drop(['Alpha-3 code','Numeric'], axis=1)\nISO_toMerge=ISO_toMerge.rename(columns={\"Alpha-2 code\": \"country code\"})\ndf = pd.merge(df, ISO_toMerge,  on ='country code',  how ='inner')\n\ndf_sample = df.sample(n=100000,random_state=Random_seed)\n\nAggregated = df[['name','Country']]\nAggregated = Aggregated.groupby(['Country']).agg(['count']).sort_values([('name', 'count')], ascending=False)\nAggregated['Percentage'] = round(Aggregated[['name']] / df.shape[0],2)\nAggregated.columns = Aggregated.columns.get_level_values(0)\nAggregated.columns = [''.join(col).strip() for col in Aggregated.columns.values]\n\nPopulation = Population.rename(columns={\"Country (or dependency)\": \"Country\"})\n\nPopulation[['Country']] = Population[['Country']].replace(\"Czech Republic (Czechia)\", \"Czechia\")\nPopulation[['Country']] = Population[['Country']].replace(\"United States\", \"United States of America\")\nPopulation[['Country']] = Population[['Country']].replace(\"United Kingdom\", \"United Kingdom of Great Britain and Northern Ireland\")\nPopulation[['Country']] = Population[['Country']].replace(\"Vietnam\", \"Viet Nam\")\nPopulation[['Country']] = Population[['Country']].replace(\"Laos\", \"Lao People Democratic Republic\")\nPopulation[['Country']] = Population[['Country']].replace(\"State of Palestine\", \"Palestine\")\nPopulation[['Country']] = Population[['Country']].replace(\"North Macedonia\", \"Republic of North Macedonia\")\nPopulation[['Country']] = Population[['Country']].replace(\"Russia\", \"Russian Federation\")\nPopulation[['Country']] = Population[['Country']].replace(\"Syria\", \"Syrian Arab Republic\")\n\nSample_Size = 1000000\n\nPopulation_Merged = pd.merge(ISO_toMerge,Population,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Population Perc']] = Population_Merged[['Population (2020)']]/Population_Merged[['Population (2020)']].sum()\n\nPopulation_Merged = pd.merge(Population_Merged,Aggregated,  on ='Country',  how ='inner')\n\nPopulation_Merged[['Sample size']] = Population_Merged['Population Perc']  / Population_Merged['name']*Population_Merged['name'].sum()*Sample_Size\n\nPopulation_toMerge = Population_Merged.loc[:, Population_Merged.columns.intersection(['Country','Sample size'])]\n\ndf = pd.merge(df,Population_toMerge,  on ='Country',  how ='inner')\n\nTotal_Probability = df[['Sample size']].sum()\n\ndf[['Sample size']] = df[['Sample size']] / Total_Probability\n\nvec = df[['Sample size']]\n\ndf_sampled = df.sample(n=Sample_Size, weights='Sample size',random_state=Random_seed)\n\ngc.collect()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Briefly to sum up what I did:\n* The NaN's were cleaned\n* I estimated elevation which had almost 85% NaNs by approximation by use of location\n* Next, I translated names of countries to more readable\n* Last and most important, I found that distribution of records do not depict real-world density. I weighted the number of spots by population of the given country","metadata":{}},{"cell_type":"markdown","source":"# 3.How are the biggest cities distributed?","metadata":{}},{"cell_type":"markdown","source":"The data, I observe, contains 11 million so-called spots/places. First, it is improtant to understand what it realls means. And then for example it can be a village, city, but aso country or even continent. To understand it I plotted records with population bigger than 10.000.000 what showed me biggest cities, but also countries and continents. On the basis of it I can define them by use of appended codes.","metadata":{}},{"cell_type":"code","source":"df_countries = df[df['feature code']==\"PCLI\"]\ndf_admin = df[df['feature code']==\"ADM1\"]\ndf_regions = df[df['feature code']==\"RGN\"]\ndf_continents = df[df['feature code']==\"CONT\"]\ndf_nature = df[df['feature code']==\"AREA\"]\n\ndf_administrative = df[df['feature class']==\"A\"]\ndf_hydrographic = df[df['feature class']==\"H\"]\ndf_area = df[df['feature class']==\"L\"]\ndf_populated = df[df['feature class']==\"P\"]\ndf_road = df[df['feature class']==\"R\"]\ndf_spot = df[df['feature class']==\"S\"]\ndf_administrative = df[df['feature class']==\"T\"]\ndf_undersea = df[df['feature class']==\"U\"]\ndf_vegetation = df[df['feature class']==\"V\"]\n\ndf['feature class'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variable 'feature code' is very granular and has 100+ levels, that's too much. I decided to differentiate records by use of 'feature class'. Let's plot them together: ","metadata":{}},{"cell_type":"code","source":"df_ten_and_more = df_populated[df.population>10000000]\ndf_five_ten = df_populated[(df.population<10000000) & (df.population>5000000)]\ndf_one_five = df_populated[(df.population<5000000) & (df.population>1000000)]\ndf_hundred_ones = df_populated[(df.population<1000000) & (df.population>100000)]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1, figsize=(15,8))\nm1 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm1.fillcontinents(color='#191919',lake_color='white') \nm1.drawmapboundary(fill_color='white')                \nm1.drawcountries(linewidth=0.3, color=\"w\")              \n\n# Plot the data\nmxy_4 = m1(df_hundred_ones[\"longitude\"].tolist(), df_hundred_ones[\"latitude\"].tolist())\nA = m1.scatter(mxy_4[0], mxy_4[1], c=\"blue\", lw=0.001, alpha=0.8, zorder=5, marker='*')\nA\n\nmxy_3 = m1(df_one_five[\"longitude\"].tolist(), df_one_five[\"latitude\"].tolist())\nB = m1.scatter(mxy_3[0], mxy_3[1], c=\"green\", lw=0.1, alpha=1, zorder=5, marker='o')\nB\n\nmxy_2 = m1(df_five_ten[\"longitude\"].tolist(), df_five_ten[\"latitude\"].tolist())\nC = m1.scatter(mxy_2[0], mxy_2[1], c=\"yellow\", lw=0.3, alpha=1, zorder=5, marker='p')\nC\n\nmxy_1 = m1(df_ten_and_more[\"longitude\"].tolist(), df_ten_and_more[\"latitude\"].tolist())\nD = m1.scatter(mxy_1[0], mxy_1[1], c=\"red\", lw=0.4, alpha=1, zorder=5, marker='s')\nD\n\nplt.legend((D,C,B,A),('>10mln','5mln-10mln', '1mln-5mln', '100k-1mln'),numpoints=1, loc=1)\n\nplt.title(\"Big cities of the world\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df\ndel Population_Merged\ndel Aggregated\ngc.collect()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finally, the question was, how the biggest cities are dispersed in the world?**\n\n**I have to admit that classification of the city's population was a bit discussable. For example, Jakarta, Cairo and Lagos were classified as cities smaller than 10 mln. It is due to the fact that indeed their metropolitan area goes far more than 10mln but they as cities are limited to central areas. What is interesting: we observe huge number of cities bigger than 100k in Europe, but barely any bigger than 5mln. Opposite behaviour is observed in Brazil and North America. India, Japan and China have both huge number of cities bigger than 100k and mega-metropolies bigger than 5mln.**","metadata":{}},{"cell_type":"markdown","source":"# 4.What is continent according to DBSCAN?\n\nAre continents differentiated reasonably or they are just humans' choice? Let's check it! For this, I ask smart guy, density-based algortihm ([more in my notebook](https://www.kaggle.com/jjmewtw/clustering-k-means-hierarchical-debscan-ema)) how (s)he would do it.","metadata":{}},{"cell_type":"code","source":"df_limited = df_sampled[['longitude','latitude']].sample(n=50000,random_state=Random_seed)\ndf_DBSCAN = StandardScaler().fit_transform(df_limited)\n\nDBcluster= DBSCAN(min_samples = 200,eps=0.4)\nDBcluster_fit = DBcluster.fit(df_DBSCAN)\ncore_samples_mask = np.zeros_like(DBcluster_fit.labels_, dtype=bool)\ncore_samples_mask[DBcluster_fit.core_sample_indices_] = True\n\nDBlabels = DBcluster_fit.labels_ \nDB_n_clusters_ = len(set(DBlabels)) \nDB_n_noise_ = list(DBlabels).count(-1)\n\nprint('Estimated number of clusters: %d' % DB_n_clusters_)\nprint('Estimated number of noise points: %d' % DB_n_noise_)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_limited = df_limited.reset_index()\ndf_cluster = pd.DataFrame(DBlabels,columns=['Cluster'])\n\ndf_limited = pd.concat([df_limited, df_cluster.reindex(df_limited.index)], axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1, figsize=(15,8))\nm5 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm5.fillcontinents(color='#191919',lake_color='white') \nm5.drawmapboundary(fill_color='white')                \nm5.drawcountries(linewidth=0.3, color=\"w\")\n\nm5.drawparallels(\n    np.arange(-60, 65, 2.),\n    color = 'black', linewidth = 0.2,\n    labels=[False, False, False, False])\nm5.drawmeridians(\n    np.arange(-180, 180, 2.),\n    color = '0.25', linewidth = 0.2,\n    labels=[False, False, False, False])\n\nm5.drawmapscale(150., -50., -10, -55,2500,units='km', fontsize=10,yoffset=None,barstyle='fancy', labelstyle='simple',\n    fillcolor1='w', fillcolor2='#000000',fontcolor='#000000',zorder=5)\n\nunique_labels = set(DBlabels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\n          \ncore_samples_mask = np.zeros_like(DBlabels, dtype=bool)\ncore_samples_mask[DBcluster.core_sample_indices_] = True\n\n# Plot the data\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n          \n    class_member_mask = (DBlabels == k)\n\n    xy = df_limited[class_member_mask & core_samples_mask]\n    mxy = m5(xy[\"longitude\"].tolist(), xy[\"latitude\"].tolist())\n    m5.scatter(mxy[0], mxy[1], c=tuple(col), lw=0.3, alpha=1, zorder=5, marker='p')\n\nplt.title(\"Continents according to very strict DBSCAN with Euclidean metric and minimal sample: 300\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, so according to this un-tuned DBSCAN there are only two continents: Americas and EurAfSia. Definitely, in this case water was at once ignored and not ignored. I mean, from one point of view I did not pre-define water as obstacle, but on other hand, anyway there are no cities in the water so type of obstacle is naturally built.","metadata":{}},{"cell_type":"code","source":"df_limited = df_sampled[['longitude','latitude']].sample(n=50000,random_state=Random_seed)\ndf_DBSCAN = StandardScaler().fit_transform(df_limited)\n\nDBcluster= DBSCAN(eps=0.15,min_samples = 170,metric=\"canberra\")\n\nDBcluster_fit = DBcluster.fit(df_DBSCAN)\ncore_samples_mask = np.zeros_like(DBcluster_fit.labels_, dtype=bool)\ncore_samples_mask[DBcluster_fit.core_sample_indices_] = True\n\nDBlabels = DBcluster_fit.labels_ \nDB_n_clusters_ = len(set(DBlabels)) \nDB_n_noise_ = list(DBlabels).count(-1)\n\ndf_limited = df_limited.reset_index()\ndf_cluster = pd.DataFrame(DBlabels,columns=['Cluster'])\ndf_limited = pd.concat([df_limited, df_cluster.reindex(df_limited.index)], axis=1)\n\nplt.figure(1, figsize=(15,8))\nm5 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm5.fillcontinents(color='#191919',lake_color='white') \nm5.drawmapboundary(fill_color='white')                \nm5.drawcountries(linewidth=0.3, color=\"w\")\n\nm5.drawparallels(\n    np.arange(-60, 65, 2.),\n    color = 'black', linewidth = 0.2,\n    labels=[False, False, False, False])\nm5.drawmeridians(\n    np.arange(-180, 180, 2.),\n    color = '0.25', linewidth = 0.2,\n    labels=[False, False, False, False])\n\nm5.drawmapscale(150., -50., -10, -55,2500,units='km', fontsize=10,yoffset=None,barstyle='fancy', labelstyle='simple',\n    fillcolor1='w', fillcolor2='#000000',fontcolor='#000000',zorder=5)\n\nunique_labels = set(DBlabels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\n          \ncore_samples_mask = np.zeros_like(DBlabels, dtype=bool)\ncore_samples_mask[DBcluster.core_sample_indices_] = True\n\n# Plot the data\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n          \n    class_member_mask = (DBlabels == k)\n\n    xy = df_limited[class_member_mask & core_samples_mask]\n    mxy = m5(xy[\"longitude\"].tolist(), xy[\"latitude\"].tolist())\n    m5.scatter(mxy[0], mxy[1], c=tuple(col), lw=0.3, alpha=1, zorder=5, marker='p')\n    \nplt.title(\"Continents according to DBSCAN with canberra metric\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright, this DBSCAN is closer to agreement with modern humankind. The huge difference comes here from the choice of metric, in this case it is Canberra metric which is a weighted version of Mahattan one. It gave me 6 continents, Europe incorporated northern part of Africa. Asia was divided into two parts.","metadata":{}},{"cell_type":"code","source":"df_limited = df_sampled[['longitude','latitude']].sample(n=50000,random_state=Random_seed)\ndf_DBSCAN = StandardScaler().fit_transform(df_limited)\n\nDBcluster= DBSCAN(metric=\"l2\",eps=0.14,min_samples = 120)\n\nDBcluster_fit = DBcluster.fit(df_DBSCAN)\ncore_samples_mask = np.zeros_like(DBcluster_fit.labels_, dtype=bool)\ncore_samples_mask[DBcluster_fit.core_sample_indices_] = True\n\nDBlabels = DBcluster_fit.labels_ \nDB_n_clusters_ = len(set(DBlabels)) \nDB_n_noise_ = list(DBlabels).count(-1)\n\ndf_limited = df_limited.reset_index()\ndf_cluster = pd.DataFrame(DBlabels,columns=['Cluster'])\ndf_limited = pd.concat([df_limited, df_cluster.reindex(df_limited.index)], axis=1)\n\nplt.figure(1, figsize=(15,8))\nm5 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,\n             lat_ts=0,resolution='c')\n\nm5.fillcontinents(color='#191919',lake_color='white') \nm5.drawmapboundary(fill_color='white')                \nm5.drawcountries(linewidth=0.3, color=\"w\")              \n\nunique_labels = set(DBlabels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\n          \ncore_samples_mask = np.zeros_like(DBlabels, dtype=bool)\ncore_samples_mask[DBcluster.core_sample_indices_] = True\n\nm5.drawparallels(\n    np.arange(-60, 65, 2.),\n    color = 'black', linewidth = 0.2,\n    labels=[False, False, False, False])\nm5.drawmeridians(\n    np.arange(-180, 180, 2.),\n    color = '0.25', linewidth = 0.2,\n    labels=[False, False, False, False])\n\nm5.drawmapscale(150., -50., -10, -55,2500,units='km', fontsize=10,yoffset=None,barstyle='fancy', labelstyle='simple',\n    fillcolor1='w', fillcolor2='#000000',fontcolor='#000000',zorder=5)\n\n# Plot the data\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n          \n    class_member_mask = (DBlabels == k)\n\n    xy = df_limited[class_member_mask & core_samples_mask]\n    mxy = m5(xy[\"longitude\"].tolist(), xy[\"latitude\"].tolist())\n    m5.scatter(mxy[0], mxy[1], c=tuple(col), lw=0.3, alpha=1, zorder=5, marker='p')\n    \nplt.title(\"Continents according to DBSCAN with L2 metric\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we received independent Africa from Eurasia. But South America was divided into multiple areas. Due is due to low density of population in the Amazonia, which divded ne continent into several regions.\n\n**Finally, the question was, is continent arbitrary definition?**\n\n**Our algorithm's answer is: definitely, continent's definition is arbitrary choice. But on other hand, even mathematical algorithms can't decide what would be the best defnitition. Some observations are clear: almost always Eurasia goes together, Africa often join them. Australia due to its distance likes to proclaim independence. Americas are always independent but they also prefer to go together. Further if we want to exclude Africa from Eurasia, then Americas will usually split to couple of regions as well.**","metadata":{}},{"cell_type":"markdown","source":"# 5.How to approximate elevation and what we can read from it?","metadata":{}},{"cell_type":"markdown","source":"What I did in the chapter '2' is approximation of the elevation by neighbouring valus. It was needed due to poor data population for elevation column. How did I approach this? I rounded latitude and longitude and imposed its average values over all NA's in the region.","metadata":{}},{"cell_type":"code","source":"norm = Normalize()\n\ndf_map = df_sampled.sample(n=10000, random_state=Random_seed)\n\nplt.figure(1, figsize=(15,8))\nm2 = Basemap(projection='merc',llcrnrlat=-60,urcrnrlat=65,llcrnrlon=-180,urcrnrlon=180,lat_ts=0,resolution='c')\n\nm2.fillcontinents(color='#C0C0C0', lake_color='#7093DB') \nm2.drawmapboundary(fill_color='white')                \nm2.drawcountries(linewidth=.75, linestyle='solid', color='#000073',antialiased=True,zorder=3) \n\nm2.drawparallels(\n    np.arange(-60, 65, 2.),\n    color = 'black', linewidth = 0.2,\n    labels=[False, False, False, False])\nm2.drawmeridians(\n    np.arange(-180, 180, 2.),\n    color = '0.25', linewidth = 0.2,\n    labels=[False, False, False, False])\n\nx,y = m2(*(df_map.longitude.values, df_map.latitude.values))\n\nnumcols, numrows = 1000, 1000\nxi = np.linspace(x.min(), x.max(), numcols)\nyi = np.linspace(y.min(), y.max(), numrows)\nxi, yi = np.meshgrid(xi, yi)\n\nx_1, y_1, z_1 = df_map.longitude.values, df_map.latitude.values, df_map.elevation_y.values\nzi = griddata((x,y),df_map.elevation_y.values,(xi, yi),method='linear')\n\ncon = m2.contourf(xi, yi, zi, zorder=4, alpha=0.7, cmap='RdPu')\n\nm2.scatter(x,y,color='#545454',edgecolor='#ffffff',alpha=.75,s=50 * norm(df_map.elevation_y),cmap='RdPu',vmin=zi.min(), vmax=zi.max(), zorder=4)\n\nm2.drawmapscale(150., -50., -10, -55,2500,units='km', fontsize=10,yoffset=None,barstyle='fancy', labelstyle='simple',\n    fillcolor1='w', fillcolor2='#000000',fontcolor='#000000',zorder=5)\n\ncbar = plt.colorbar(con, orientation='horizontal', fraction=.057, pad=0.05)\ncbar.set_label(\"Regional elevation - m\")\n\nplt.title(\"Elevation of the region with mountains marked\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finally: the question was: How to approximate elevation and what we can read from it?**\n\n**First, please pay attention that this map was built by use of approximated elevation, namely 85% of the data was preliminarly in NA class. I would say it is quite good result for this. We easily can observe the most relevant mountain ranges of the wrold with generally plotted elevation along. I can say that application of rounded longitude/latitude with apporximation to full degree is absolutely enough to receive reasonable results.**","metadata":{}}]}